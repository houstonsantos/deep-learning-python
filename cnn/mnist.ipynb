{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "47c239c36de4969fcd12702dcc064548aea50277e5acb96ec5b736153390dbfd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "source": [
    "# Rede Neural - Convolucional"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando base de dados\n",
    "(X_treinamento, y_treinamento), (X_teste, y_teste) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X_treinamento.shape, y_treinamento.shape, X_teste.shape, y_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando conversão, reshape e scaling dos valores\n",
    "previsores_treinamento = X_treinamento.reshape(X_treinamento.shape[0], 28, 28, 1)\n",
    "previsores_treinamento = previsores_treinamento.astype('float32')\n",
    "previsores_treinamento /= 255\n",
    "\n",
    "previsores_teste = X_teste.reshape(X_teste.shape[0], 28, 28, 1)\n",
    "previsores_teste = previsores_teste.astype('float32')\n",
    "previsores_teste /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "previsores_treinamento.shape, previsores_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando One Hot Encoding nas classes \n",
    "classe_treinamento = np_utils.to_categorical(y_treinamento, 10)\n",
    "classe_teste = np_utils.to_categorical(y_teste, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((60000,), (10000,))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "y_treinamento.shape, y_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "classe_treinamento.shape, classe_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando estrutura da rede neural\n",
    "classificador = Sequential()\n",
    "classificador.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1), activation = 'relu'))\n",
    "classificador.add(BatchNormalization())\n",
    "classificador.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classificador.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classificador.add(BatchNormalization())\n",
    "classificador.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classificador.add(Flatten())\n",
    "classificador.add(Dense(units = 128, activation = 'relu'))\n",
    "classificador.add(Dropout(0.2))\n",
    "classificador.add(Dense(units = 128, activation = 'relu'))\n",
    "classificador.add(Dropout(0.2))\n",
    "classificador.add(Dense(units = 10, activation = 'softmax'))\n",
    "# Compilando rede neural\n",
    "classificador.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 26, 26, 32)        128       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 11, 11, 32)        9248      \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 11, 11, 32)        128       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 5, 5, 32)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 800)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               102528    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 130,154\nTrainable params: 130,026\nNon-trainable params: 128\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Resumo do modelo criado\n",
    "classificador.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "469/469 [==============================] - 44s 91ms/step - loss: 0.4297 - accuracy: 0.8663 - val_loss: 0.1897 - val_accuracy: 0.9415\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.0646 - accuracy: 0.9808 - val_loss: 0.0392 - val_accuracy: 0.9867\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x167212760>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Treinamento e teste do modelo\n",
    "classificador.fit(\n",
    "    previsores_treinamento, \n",
    "    classe_treinamento, \n",
    "    batch_size = 128, \n",
    "    epochs = 2, \n",
    "    validation_data = (previsores_teste, classe_teste)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0392 - accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "# Accuracy usando keras\n",
    "resultado = classificador.evaluate(previsores_teste, classe_teste)"
   ]
  },
  {
   "source": [
    "# Rede Neural - Convolucional\n",
    "## Validação Cruzada com K-fold "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo semente\n",
    "seed = 5\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando base de dados\n",
    "(X, y), (X_teste, y_teste) = mnist.load_data()\n",
    "\n",
    "# Realizando conversão, reshape e scaling dos valores\n",
    "previsores = X.reshape(X.shape[0], 28, 28, 1)\n",
    "previsores = previsores.astype('float32')\n",
    "previsores /= 255\n",
    "\n",
    "# Realizando One Hot Encoding nas classes \n",
    "classe = np_utils.to_categorical(y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed)\n",
    "resultados = []\n",
    "\n",
    "#b = np.zeros(shape = (classe.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 14s 36ms/step - loss: 0.4893 - accuracy: 0.8633\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 14s 39ms/step - loss: 0.0847 - accuracy: 0.9761\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 13s 35ms/step - loss: 0.0520 - accuracy: 0.9842\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 14s 38ms/step - loss: 0.0328 - accuracy: 0.9912\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 14s 36ms/step - loss: 0.0252 - accuracy: 0.9921\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0630 - accuracy: 0.9807\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 14s 35ms/step - loss: 0.5244 - accuracy: 0.8506\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 13s 35ms/step - loss: 0.0913 - accuracy: 0.9739\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 15s 40ms/step - loss: 0.0524 - accuracy: 0.9851\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0389 - accuracy: 0.9884\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0267 - accuracy: 0.9933\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0641 - accuracy: 0.9807\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 13s 33ms/step - loss: 0.4677 - accuracy: 0.8681\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 13s 35ms/step - loss: 0.0915 - accuracy: 0.9736\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0551 - accuracy: 0.9845\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 13s 35ms/step - loss: 0.0400 - accuracy: 0.9885\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 14s 37ms/step - loss: 0.0286 - accuracy: 0.9918\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0510 - accuracy: 0.9848\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 14s 36ms/step - loss: 0.4964 - accuracy: 0.8576\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0850 - accuracy: 0.9754\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0524 - accuracy: 0.9850\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0339 - accuracy: 0.9901\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 13s 35ms/step - loss: 0.0275 - accuracy: 0.9920\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0545 - accuracy: 0.9832\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.4861 - accuracy: 0.8641\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0869 - accuracy: 0.9750\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0508 - accuracy: 0.9851\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0374 - accuracy: 0.9887\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 13s 34ms/step - loss: 0.0257 - accuracy: 0.9925\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0572 - accuracy: 0.9831\n"
     ]
    }
   ],
   "source": [
    "for indice_treinamento, indice_teste in kfold.split(previsores, np.zeros(shape = (classe.shape[0], 1))):\n",
    "    #print('Índices treinamento: ', indice_treinamento, 'Índice teste', indice_teste)\n",
    "\n",
    "    # Criando estrutura da rede neural\n",
    "    classificador = Sequential()\n",
    "    classificador.add(Conv2D(32, (3,3), input_shape=(28,28,1), activation = 'relu'))\n",
    "    classificador.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    classificador.add(Flatten())\n",
    "    classificador.add(Dense(units = 128, activation = 'relu'))\n",
    "    classificador.add(Dense(units = 10, activation = 'softmax'))\n",
    "    # Compilando rede neural\n",
    "    classificador.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    # Treinamento e teste do modelo\n",
    "    classificador.fit(previsores[indice_treinamento], classe[indice_treinamento], batch_size = 128, epochs = 5)\n",
    "\n",
    "    # Accuracy usando keras\n",
    "    precisao = classificador.evaluate(previsores[indice_teste], classe[indice_teste])\n",
    "    resultados.append(precisao[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.9806666374206543,\n",
       " 0.9806666374206543,\n",
       " 0.9848333597183228,\n",
       " 0.9831666946411133,\n",
       " 0.9830833077430725]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9824833273887634"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# media = resultados.mean()\n",
    "media = sum(resultados) / len(resultados)\n",
    "media"
   ]
  },
  {
   "source": [
    "# Rede Neural - Convolucional\n",
    "## Augumentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      " 469/4687 [==>...........................] - ETA: 4:49 - loss: 0.8607 - accuracy: 0.7228WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 23435 batches). You may need to use the repeat() function when building your dataset.\n",
      "4687/4687 [==============================] - 34s 7ms/step - loss: 0.5866 - accuracy: 0.8129 - val_loss: 0.1743 - val_accuracy: 0.9486\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x162c85b50>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# Criando estrutura da rede neural\n",
    "classificador = Sequential()\n",
    "classificador.add(Conv2D(32, (3,3), input_shape=(28, 28, 1), activation = 'relu'))\n",
    "classificador.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classificador.add(Flatten())\n",
    "classificador.add(Dense(units = 128, activation = 'relu'))\n",
    "classificador.add(Dense(units = 10, activation = 'softmax'))\n",
    "# Compilando rede neural\n",
    "classificador.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "gerador_treinamento = ImageDataGenerator(\n",
    "    rotation_range = 7,\n",
    "    horizontal_flip = True,\n",
    "    shear_range = 0.2,\n",
    "    height_shift_range = 0.07,\n",
    "    zoom_range = 0.2\n",
    ")\n",
    "\n",
    "# Parâmetros do Augumentation\n",
    "gerador_teste = ImageDataGenerator()\n",
    "base_treinamento = gerador_treinamento.flow(previsores_treinamento, classe_treinamento, batch_size = 128)\n",
    "base_teste = gerador_teste.flow(previsores_teste, classe_teste, batch_size = 128)\n",
    "\n",
    "# Treinamento e teste do modelo\n",
    "classificador.fit_generator(\n",
    "    base_treinamento, \n",
    "    steps_per_epoch = 600000 // 128,\n",
    "    epochs = 5, \n",
    "    validation_data = base_teste,\n",
    "    validation_steps = 10000 // 128\n",
    ")"
   ]
  }
 ]
}